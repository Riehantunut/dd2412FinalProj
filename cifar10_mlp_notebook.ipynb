{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import NamedTuple\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "class PermutationSpec(NamedTuple):\n",
    "  perm_to_axes: dict\n",
    "  axes_to_perm: dict\n",
    "\n",
    "def permutation_spec_from_axes_to_perm(axes_to_perm: dict) -> PermutationSpec:\n",
    "  perm_to_axes = defaultdict(list)\n",
    "  for wk, axis_perms in axes_to_perm.items():\n",
    "    for axis, perm in enumerate(axis_perms):\n",
    "      if perm is not None:\n",
    "        perm_to_axes[perm].append((wk, axis))\n",
    "  return PermutationSpec(perm_to_axes=dict(perm_to_axes), axes_to_perm=axes_to_perm)\n",
    "\n",
    "def mlp_permutation_spec(num_hidden_layers: int) -> PermutationSpec:\n",
    "  \"\"\"We assume that one permutation cannot appear in two axes of the same weight array.\"\"\"\n",
    "  assert num_hidden_layers >= 1\n",
    "  return permutation_spec_from_axes_to_perm({\n",
    "      \"Dense_0/kernel\": (None, \"P_0\"),\n",
    "      **{f\"Dense_{i}/kernel\": (f\"P_{i-1}\", f\"P_{i}\")\n",
    "         for i in range(1, num_hidden_layers)},\n",
    "      **{f\"Dense_{i}/bias\": (f\"P_{i}\", )\n",
    "         for i in range(num_hidden_layers)},\n",
    "      f\"Dense_{num_hidden_layers}/kernel\": (f\"P_{num_hidden_layers-1}\", None),\n",
    "      f\"Dense_{num_hidden_layers}/bias\": (None, ),\n",
    "  })\n",
    "\n",
    "def get_permuted_param(ps: PermutationSpec, perm, k: str, params, except_axis=None):\n",
    "  \"\"\"Get parameter `k` from `params`, with the permutations applied.\"\"\"\n",
    "  w = params[k]\n",
    "  for axis, p in enumerate(ps.axes_to_perm[k]):\n",
    "    # Skip the axis we're trying to permute.\n",
    "    if axis == except_axis:\n",
    "      continue\n",
    "\n",
    "    # None indicates that there is no permutation relevant to that axis.\n",
    "    if p is not None:\n",
    "      w = jnp.take(w, perm[p], axis=axis)\n",
    "\n",
    "  return w\n",
    "\n",
    "def apply_permutation(ps: PermutationSpec, perm, params):\n",
    "  \"\"\"Apply a `perm` to `params`.\"\"\"\n",
    "  return {k: get_permuted_param(ps, perm, k, params) for k in params.keys()}\n",
    "\n",
    "def weight_matching(rng, ps: PermutationSpec, params_a, params_b, max_iter=100, init_perm=None):\n",
    "  \"\"\"Find a permutation of `params_b` to make them match `params_a`.\"\"\"\n",
    "  perm_sizes = {p: params_a[axes[0][0]].shape[axes[0][1]] for p, axes in ps.perm_to_axes.items()}\n",
    "\n",
    "  perm = {p: jnp.arange(n) for p, n in perm_sizes.items()} if init_perm is None else init_perm\n",
    "  perm_names = list(perm.keys())\n",
    "\n",
    "  for iteration in range(max_iter):\n",
    "    progress = False\n",
    "    for p_ix in random.permutation(rngmix(rng, iteration), len(perm_names)):\n",
    "      p = perm_names[p_ix]\n",
    "      n = perm_sizes[p]\n",
    "      A = jnp.zeros((n, n))\n",
    "      for wk, axis in ps.perm_to_axes[p]:\n",
    "        w_a = params_a[wk]\n",
    "        w_b = get_permuted_param(ps, perm, wk, params_b, except_axis=axis)\n",
    "        w_a = jnp.moveaxis(w_a, axis, 0).reshape((n, -1))\n",
    "        w_b = jnp.moveaxis(w_b, axis, 0).reshape((n, -1))\n",
    "        A += w_a @ w_b.T\n",
    "\n",
    "      ri, ci = linear_sum_assignment(A, maximize=True)\n",
    "      assert (ri == jnp.arange(len(ri))).all()\n",
    "\n",
    "      oldL = jnp.vdot(A, jnp.eye(n)[perm[p]])\n",
    "      newL = jnp.vdot(A, jnp.eye(n)[ci, :])\n",
    "      print(f\"{iteration}/{p}: {newL - oldL}\")\n",
    "      progress = progress or newL > oldL + 1e-12\n",
    "\n",
    "      perm[p] = jnp.array(ci)\n",
    "\n",
    "    if not progress:\n",
    "      break\n",
    "\n",
    "  return perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import re\n",
    "import time\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from flax import traverse_util\n",
    "from flax.core import freeze, unfreeze\n",
    "from jax import random, tree_map\n",
    "from jax.tree_util import tree_reduce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rngmix = lambda rng, x: random.fold_in(rng, hash(x))\n",
    "\n",
    "class RngPooper:\n",
    "  \"\"\"A stateful wrapper around stateless random.PRNGKey's.\"\"\"\n",
    "\n",
    "  def __init__(self, init_rng):\n",
    "    self.rng = init_rng\n",
    "\n",
    "  def poop(self):\n",
    "    self.rng, rng_key = random.split(self.rng)\n",
    "    return rng_key\n",
    "\n",
    "def l1prox(x, alpha):\n",
    "  return jnp.sign(x) * jnp.maximum(0, jnp.abs(x) - alpha)\n",
    "\n",
    "def flatten_params(params):\n",
    "  return {\"/\".join(k): v for k, v in traverse_util.flatten_dict(unfreeze(params)).items()}\n",
    "\n",
    "def unflatten_params(flat_params):\n",
    "  return freeze(\n",
    "      traverse_util.unflatten_dict({tuple(k.split(\"/\")): v\n",
    "                                    for k, v in flat_params.items()}))\n",
    "\n",
    "def merge_params(a, b):\n",
    "  return unflatten_params({**a, **b})\n",
    "\n",
    "def kmatch(pattern, key):\n",
    "  regex = \"^\"\n",
    "  i = 0\n",
    "  while i < len(pattern):\n",
    "    if pattern[i] == \"*\":\n",
    "      if i + 1 < len(pattern) and pattern[i + 1] == \"*\":\n",
    "        regex += \"(.*)\"\n",
    "        i += 2\n",
    "      else:\n",
    "        regex += \"([^\\/]*)\"\n",
    "        i += 1\n",
    "    else:\n",
    "      regex += pattern[i]\n",
    "      i += 1\n",
    "  regex += \"$\"\n",
    "  return re.fullmatch(regex, key)\n",
    "\n",
    "assert kmatch(\"*\", \"a\") is not None\n",
    "assert kmatch(\"*\", \"a\").group(0) == \"a\"\n",
    "assert kmatch(\"*\", \"a\").group(1) == \"a\"\n",
    "assert kmatch(\"abc\", \"def\") is None\n",
    "assert kmatch(\"abc/*/ghi\", \"abc/def/ghi\").group(1) == \"def\"\n",
    "assert kmatch(\"abc/**/jkl\", \"abc/def/ghi/jkl\").group(1) == \"def/ghi\"\n",
    "assert kmatch(\"abc/*/jkl\", \"abc/def/ghi/jkl\") is None\n",
    "assert kmatch(\"**/*\", \"abc/def/ghi/jkl\").group(1) == \"abc/def/ghi\"\n",
    "assert kmatch(\"**/*\", \"abc/def/ghi/jkl\").group(2) == \"jkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Train an MLP on Cifar10 on one random seed. Serialize the model for\n",
    "interpolation downstream.\"\"\"\n",
    "import argparse\n",
    "\n",
    "import augmax\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "from jax import jit, random, tree_map, value_and_grad, vmap\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "activation = nn.relu\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = jnp.reshape(x, (-1, 32 * 32, 3))\n",
    "    x = nn.Dense(512)(x)\n",
    "    x = activation(x)\n",
    "    x = nn.Dense(512)(x)\n",
    "    x = activation(x)\n",
    "    x = nn.Dense(512)(x)\n",
    "    x = activation(x)\n",
    "    x = nn.Dense(10)(x)\n",
    "    x = nn.log_softmax(x)\n",
    "    return x\n",
    "\n",
    "def make_stuff(model):\n",
    "  train_transform = augmax.Chain(\n",
    "      # augmax does not seem to support random crops with padding. See https://github.com/khdlr/augmax/issues/6.\n",
    "      augmax.RandomSizedCrop(32, 32, zoom_range=(0.8, 1.2)),\n",
    "      augmax.HorizontalFlip(),\n",
    "      augmax.Rotate(),\n",
    "  )\n",
    "  # Applied to all input images, test and train.\n",
    "  normalize_transform = augmax.Chain(augmax.ByteToFloat(), augmax.Normalize())\n",
    "\n",
    "  @jit\n",
    "  def batch_eval(params, images_u8, labels):\n",
    "    images_f32 = vmap(normalize_transform)(None, images_u8)\n",
    "    y_onehot = jax.nn.one_hot(labels, 10)\n",
    "    logits = model.apply({\"params\": params}, images_f32)\n",
    "    l = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=y_onehot))\n",
    "    num_correct = jnp.sum(jnp.argmax(logits, axis=-1) == labels)\n",
    "    return l, {\"num_correct\": num_correct}\n",
    "\n",
    "  @jit\n",
    "  def step(rng, train_state, images, labels):\n",
    "    images_transformed = vmap(train_transform)(random.split(rng, images.shape[0]), images)\n",
    "    (l, info), g = value_and_grad(batch_eval, has_aux=True)(train_state.params, images_transformed,\n",
    "                                                            labels)\n",
    "    return train_state.apply_gradients(grads=g), {\"batch_loss\": l, **info}\n",
    "\n",
    "  def dataset_loss_and_accuracy(params, dataset, batch_size: int):\n",
    "    num_examples = dataset[\"images_u8\"].shape[0]\n",
    "    assert num_examples % batch_size == 0\n",
    "    num_batches = num_examples // batch_size\n",
    "    batch_ix = jnp.arange(num_examples).reshape((num_batches, batch_size))\n",
    "    # Can't use vmap or run in a single batch since that overloads GPU memory.\n",
    "    losses, infos = zip(*[\n",
    "        batch_eval(\n",
    "            params,\n",
    "            dataset[\"images_u8\"][batch_ix[i, :], :, :, :],\n",
    "            dataset[\"labels\"][batch_ix[i, :]],\n",
    "        ) for i in range(num_batches)\n",
    "    ])\n",
    "    return (\n",
    "        jnp.sum(batch_size * jnp.array(losses)) / num_examples,\n",
    "        sum(x[\"num_correct\"] for x in infos) / num_examples,\n",
    "    )\n",
    "\n",
    "  return {\n",
    "      \"train_transform\": train_transform,\n",
    "      \"normalize_transform\": normalize_transform,\n",
    "      \"batch_eval\": batch_eval,\n",
    "      \"step\": step,\n",
    "      \"dataset_loss_and_accuracy\": dataset_loss_and_accuracy,\n",
    "  }\n",
    "\n",
    "def load_cifar10():\n",
    "  \"\"\"Return the training and test datasets, as jnp.array's.\"\"\"\n",
    "  train_ds_images_u8, train_ds_labels = tfds.as_numpy(\n",
    "      tfds.load(\"cifar10\", split=\"train\", batch_size=-1, as_supervised=True))\n",
    "  test_ds_images_u8, test_ds_labels = tfds.as_numpy(\n",
    "      tfds.load(\"cifar10\", split=\"test\", batch_size=-1, as_supervised=True))\n",
    "  train_ds = {\"images_u8\": train_ds_images_u8, \"labels\": train_ds_labels}\n",
    "  test_ds = {\"images_u8\": test_ds_images_u8, \"labels\": test_ds_labels}\n",
    "  return train_ds, test_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "seed = 12421\n",
    "config_optimizer = \"adam\" #\"sgd\"\n",
    "learning_rate = 1e-3\n",
    "\n",
    "runs_to_collect = 2 #Stan's new stuff\n",
    "\n",
    "rng = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Starting run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 15:10:15.288519: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_ds labels hash -7398819420921030134\n",
      "test_ds labels hash 3233336346247759408\n",
      "num_train_examples 50000\n",
      "num_test_examples 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(100, 10), (100, 1024, 10)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/util.py:222\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m   \u001b[39mreturn\u001b[39;00m cached(config\u001b[39m.\u001b[39;49m_trace_context(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/util.py:215\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcached\u001b[39m(_, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 215\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/lax/lax.py:147\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m@cache\u001b[39m()\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_broadcast_shapes_cached\u001b[39m(\u001b[39m*\u001b[39mshapes: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]:\n\u001b[0;32m--> 147\u001b[0m   \u001b[39mreturn\u001b[39;00m _broadcast_shapes_uncached(\u001b[39m*\u001b[39;49mshapes)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/lax/lax.py:163\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(100, 10), (100, 1024, 10)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m   images_u8 \u001b[39m=\u001b[39m train_ds[\u001b[39m\"\u001b[39m\u001b[39mimages_u8\u001b[39m\u001b[39m\"\u001b[39m][p, :, :, :]\n\u001b[1;32m     59\u001b[0m   labels \u001b[39m=\u001b[39m train_ds[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m][p]\n\u001b[0;32m---> 60\u001b[0m   train_state, info \u001b[39m=\u001b[39m stuff[\u001b[39m\"\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m\"\u001b[39;49m](batch_rngs[i], train_state, images_u8, labels)\n\u001b[1;32m     62\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(batch_size \u001b[39m*\u001b[39m x[\u001b[39m\"\u001b[39m\u001b[39mbatch_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m infos) \u001b[39m/\u001b[39m num_train_examples\n\u001b[1;32m     63\u001b[0m train_accuracy \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(x[\u001b[39m\"\u001b[39m\u001b[39mnum_correct\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m infos) \u001b[39m/\u001b[39m num_train_examples\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [3], line 61\u001b[0m, in \u001b[0;36mmake_stuff.<locals>.step\u001b[0;34m(rng, train_state, images, labels)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m@jit\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(rng, train_state, images, labels):\n\u001b[1;32m     60\u001b[0m   images_transformed \u001b[39m=\u001b[39m vmap(train_transform)(random\u001b[39m.\u001b[39msplit(rng, images\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), images)\n\u001b[0;32m---> 61\u001b[0m   (l, info), g \u001b[39m=\u001b[39m value_and_grad(batch_eval, has_aux\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(train_state\u001b[39m.\u001b[39;49mparams, images_transformed,\n\u001b[1;32m     62\u001b[0m                                                           labels)\n\u001b[1;32m     63\u001b[0m   \u001b[39mreturn\u001b[39;00m train_state\u001b[39m.\u001b[39mapply_gradients(grads\u001b[39m=\u001b[39mg), {\u001b[39m\"\u001b[39m\u001b[39mbatch_loss\u001b[39m\u001b[39m\"\u001b[39m: l, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minfo}\n",
      "    \u001b[0;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn [3], line 54\u001b[0m, in \u001b[0;36mmake_stuff.<locals>.batch_eval\u001b[0;34m(params, images_u8, labels)\u001b[0m\n\u001b[1;32m     52\u001b[0m y_onehot \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mone_hot(labels, \u001b[39m10\u001b[39m)\n\u001b[1;32m     53\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mapply({\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: params}, images_f32)\n\u001b[0;32m---> 54\u001b[0m l \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(optax\u001b[39m.\u001b[39;49msoftmax_cross_entropy(logits\u001b[39m=\u001b[39;49mlogits, labels\u001b[39m=\u001b[39;49my_onehot))\n\u001b[1;32m     55\u001b[0m num_correct \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39msum(jnp\u001b[39m.\u001b[39margmax(logits, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m labels)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m l, {\u001b[39m\"\u001b[39m\u001b[39mnum_correct\u001b[39m\u001b[39m\"\u001b[39m: num_correct}\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/optax/_src/loss.py:170\u001b[0m, in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39m\"\"\"Computes the softmax cross entropy between sets of logits and labels.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \n\u001b[1;32m    151\u001b[0m \u001b[39mMeasures the probability error in discrete classification tasks in which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39m  distributions, with shape `[...]`.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m chex\u001b[39m.\u001b[39massert_type([logits], \u001b[39mfloat\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mjnp\u001b[39m.\u001b[39msum(labels \u001b[39m*\u001b[39;49m jax\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mlog_softmax(logits, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:4710\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4708\u001b[0m args \u001b[39m=\u001b[39m (other, \u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m swap \u001b[39melse\u001b[39;00m (\u001b[39mself\u001b[39m, other)\n\u001b[1;32m   4709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m-> 4710\u001b[0m   \u001b[39mreturn\u001b[39;00m binary_op(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   4711\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m   4712\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsupported operand type(s) for \u001b[39m\u001b[39m{\u001b[39;00mopchar\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4713\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(args[\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/numpy/ufuncs.py:83\u001b[0m, in \u001b[0;36m_maybe_bool_binop.<locals>.fn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(x1, x2):\n\u001b[0;32m---> 83\u001b[0m   x1, x2 \u001b[39m=\u001b[39m _promote_args(numpy_fn\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m, x1, x2)\n\u001b[1;32m     84\u001b[0m   \u001b[39mreturn\u001b[39;00m lax_fn(x1, x2) \u001b[39mif\u001b[39;00m x1\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mbool_ \u001b[39melse\u001b[39;00m bool_lax_fn(x1, x2)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/numpy/util.py:356\u001b[0m, in \u001b[0;36m_promote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    354\u001b[0m _check_arraylike(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[1;32m    355\u001b[0m _check_no_float0s(fun_name, \u001b[39m*\u001b[39margs)\n\u001b[0;32m--> 356\u001b[0m \u001b[39mreturn\u001b[39;00m _promote_shapes(fun_name, \u001b[39m*\u001b[39;49m_promote_dtypes(\u001b[39m*\u001b[39;49margs))\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/numpy/util.py:249\u001b[0m, in \u001b[0;36m_promote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mjax_numpy_rank_promotion \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    248\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 249\u001b[0m result_rank \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(lax\u001b[39m.\u001b[39;49mbroadcast_shapes(\u001b[39m*\u001b[39;49mshapes))\n\u001b[1;32m    250\u001b[0m \u001b[39mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (result_rank \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(shp)) \u001b[39m+\u001b[39m shp)\n\u001b[1;32m    251\u001b[0m         \u001b[39mfor\u001b[39;00m arg, shp \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jax_env/lib/python3.9/site-packages/jax/_src/lax/lax.py:163\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    161\u001b[0m result_shape \u001b[39m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    162\u001b[0m \u001b[39mif\u001b[39;00m result_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(shapes)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[39mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(100, 10), (100, 1024, 10)]"
     ]
    }
   ],
   "source": [
    "# storing model params after training\n",
    "flattened_models_list = []\n",
    "\n",
    "for run_i in range(runs_to_collect):\n",
    "\n",
    "  rng = random.PRNGKey(seed+run_i) # chaning the seed each run \n",
    "  rngmix = lambda rng, x: random.fold_in(rng, hash(x))\n",
    "\n",
    "  model = MLPModel()\n",
    "  stuff = make_stuff(model)\n",
    "\n",
    "  print(\"--------------------------\")\n",
    "  print(f\"Starting run {run_i}\")\n",
    "\n",
    "\n",
    "  train_ds, test_ds = load_cifar10()\n",
    "  print(\"train_ds labels hash\", hash(np.array(train_ds[\"labels\"]).tobytes()))\n",
    "  print(\"test_ds labels hash\", hash(np.array(test_ds[\"labels\"]).tobytes()))\n",
    "\n",
    "  num_train_examples = train_ds[\"images_u8\"].shape[0]\n",
    "  num_test_examples = test_ds[\"images_u8\"].shape[0]\n",
    "  assert num_train_examples % batch_size == 0\n",
    "  print(\"num_train_examples\", num_train_examples)\n",
    "  print(\"num_test_examples\", num_test_examples)\n",
    "\n",
    "  if config_optimizer == \"sgd\":\n",
    "    lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "        init_value=1e-6,\n",
    "        peak_value=learning_rate,\n",
    "        warmup_steps=num_train_examples // batch_size,\n",
    "        # Confusingly, `decay_steps` is actually the total number of steps,\n",
    "        # including the warmup.\n",
    "        decay_steps=num_epochs * (num_train_examples // batch_size),\n",
    "    )\n",
    "    #tx = optax.sgd(lr_schedule, momentum=0.9)\n",
    "    tx = optax.chain(optax.add_decayed_weights(5e-4), optax.sgd(lr_schedule, momentum=0.9))\n",
    "    \n",
    "  elif config_optimizer == \"adam\":\n",
    "    tx = optax.adam(learning_rate)\n",
    "\n",
    "  else:\n",
    "    tx = optax.adamw(learning_rate, weight_decay=1e-4)\n",
    "\n",
    "  train_state = TrainState.create(\n",
    "      apply_fn=model.apply,\n",
    "      params=model.init(rngmix(rng, f\"init{run_i}\"), jnp.zeros((1, 32, 32, 3)))[\"params\"],\n",
    "      tx=tx,\n",
    "  )\n",
    "\n",
    "  for epoch in tqdm(range(num_epochs)):\n",
    "    infos = []\n",
    "  \n",
    "    batch_ix = random.permutation(rngmix(rng, f\"epoch-{epoch}\"), num_train_examples).reshape(\n",
    "        (-1, batch_size))\n",
    "    batch_rngs = random.split(rngmix(rng, f\"batch_rngs-{epoch}\"), batch_ix.shape[0])\n",
    "    for i in range(batch_ix.shape[0]):\n",
    "      p = batch_ix[i, :]\n",
    "      images_u8 = train_ds[\"images_u8\"][p, :, :, :]\n",
    "      labels = train_ds[\"labels\"][p]\n",
    "      train_state, info = stuff[\"step\"](batch_rngs[i], train_state, images_u8, labels)\n",
    "\n",
    "    train_loss = sum(batch_size * x[\"batch_loss\"] for x in infos) / num_train_examples\n",
    "    train_accuracy = sum(x[\"num_correct\"] for x in infos) / num_train_examples\n",
    "\n",
    "    \n",
    "    actest_loss, test_accuracy = stuff[\"dataset_loss_and_accuracy\"](train_state.params, test_ds,\n",
    "                                                                    1000)\n",
    "\n",
    "    params_l2 = tree_map(lambda x: jnp.sqrt(jnp.sum(x**2)),\n",
    "                          flatten_params({\"params_l2\": train_state.params}))\n",
    "    \n",
    "    \n",
    "  # summary\n",
    "  print(\"-----------------\")\n",
    "  print(f\"Run {run_i} DONE!\")\n",
    "  test_loss, test_accuracy = stuff[\"dataset_loss_and_accuracy\"](train_state.params, test_ds,\n",
    "                                                                1000)\n",
    "  train_loss, train_accuracy = stuff[\"dataset_loss_and_accuracy\"](train_state.params, train_ds,\n",
    "                                                                50_000)\n",
    "  print(f\"Run={run_i} test loss={test_loss} test accuracy={test_accuracy} train loss={train_loss} train accuracy={train_accuracy}\")\n",
    "\n",
    "  flattened_models_list.append(flatten_params(train_state.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flattening params\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(flattened_models_list) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      3\u001b[0m flattened_model1 \u001b[39m=\u001b[39m flattened_models_list[\u001b[39m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m flattened_model2 \u001b[39m=\u001b[39m flattened_models_list[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# flattening params\n",
    "assert len(flattened_models_list) >= 2\n",
    "flattened_model1 = flattened_models_list[0]\n",
    "flattened_model2 = flattened_models_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_spec = mlp_permutation_spec(3)\n",
    "final_permutation = weight_matching(random.PRNGKey(seed), permutation_spec,\n",
    "                                    flattened_model1, flattened_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the permutation to the second model\n",
    "flattened_model1_permuted = unflatten_params(\n",
    "        apply_permutation(permutation_spec, final_permutation, flattened_model1)\n",
    "        )\n",
    "flattened_model2_permuted = unflatten_params(\n",
    "        apply_permutation(permutation_spec, final_permutation, flattened_model2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_model1_permuted_flat = flatten_params(flattened_model1_permuted)\n",
    "flattened_model2_permuted_flat = flatten_params(flattened_model2_permuted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all in long vectors\n",
    "keys = flattened_model1.keys()\n",
    "\n",
    "v1 = jnp.concatenate([flattened_model1[key].reshape([-1]) for key in keys],axis=0)\n",
    "v2 = jnp.concatenate([flattened_model2[key].reshape([-1]) for key in keys],axis=0)\n",
    "v1_perm = jnp.concatenate([flattened_model1_permuted_flat[key].reshape([-1]) for key in keys],axis=0)\n",
    "v2_perm = jnp.concatenate([flattened_model2_permuted_flat[key].reshape([-1]) for key in keys],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_combination(factor1,vector1,factor2,vector2):\n",
    "  return dict(\n",
    "      [(key,factor1*vector1[key]+factor2*vector2[key]) for key in vector1.keys()]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(0.0,1.0,10)\n",
    "\n",
    "start_vector = flattened_model1\n",
    "stop_vector = flattened_model2\n",
    "name = \"Model 1 -> Model 2\"\n",
    "\n",
    "test_losses_list = []\n",
    "test_accs_list = []\n",
    "train_losses_list = []\n",
    "train_accs_list = []\n",
    "\n",
    "for i,t in enumerate(ts):\n",
    "  new_params = get_linear_combination(\n",
    "      (1.0-t), start_vector, t, stop_vector\n",
    "  )\n",
    "  test_loss, test_accuracy = stuff[\"dataset_loss_and_accuracy\"](unflatten_params(new_params), test_ds,\n",
    "                                                                  1000)\n",
    "  test_losses_list.append(test_loss)\n",
    "  test_accs_list.append(test_accuracy)\n",
    "\n",
    "  train_loss, train_accuracy = stuff[\"dataset_loss_and_accuracy\"](unflatten_params(new_params), train_ds,\n",
    "                                                                  50_000)\n",
    "  train_losses_list.append(train_loss)\n",
    "  train_accs_list.append(train_accuracy)\n",
    "\n",
    "  print(i,t,test_loss)\n",
    "\n",
    "plt.figure(figsize=(6,5),dpi=75)\n",
    "plt.gca().patch.set_facecolor('white')\n",
    "plt.title(f\"Linear weight interpolation\\n{name}\",fontsize=16)\n",
    "plt.plot(ts,train_losses_list,label=\"train\",marker=\"o\",color=\"crimson\",linewidth=2)\n",
    "plt.plot(ts,test_losses_list,label=\"test\",marker=\"o\",color=\"navy\",linewidth=2)\n",
    "plt.xlabel(\"Interpolation coeff\",fontsize=16)\n",
    "plt.ylabel(\"Loss\",fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(0.0,1.0,10)\n",
    "\n",
    "start_vector = flattened_model1\n",
    "stop_vector = flattened_model2_permuted_flat\n",
    "name = \"Model 1 -> (Model 2 + permutations)\"\n",
    "\n",
    "test_losses_list = []\n",
    "test_accs_list = []\n",
    "train_losses_list = []\n",
    "train_accs_list = []\n",
    "\n",
    "for i,t in enumerate(ts):\n",
    "  new_params = get_linear_combination(\n",
    "      (1.0-t), start_vector, t, stop_vector\n",
    "  )\n",
    "  test_loss, test_accuracy = stuff[\"dataset_loss_and_accuracy\"](unflatten_params(new_params), test_ds,\n",
    "                                                                  1000)\n",
    "  test_losses_list.append(test_loss)\n",
    "  test_accs_list.append(test_accuracy)\n",
    "\n",
    "  train_loss, train_accuracy = stuff[\"dataset_loss_and_accuracy\"](unflatten_params(new_params), train_ds,\n",
    "                                                                  50_000)\n",
    "  train_losses_list.append(train_loss)\n",
    "  train_accs_list.append(train_accuracy)\n",
    "\n",
    "  print(i,t,test_loss)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5),dpi=75)\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.title(f\"Linear weight interpolation\\n{name}\",fontsize=16)\n",
    "plt.plot(ts,train_losses_list,label=\"train\",marker=\"o\",color=\"crimson\",linewidth=2)\n",
    "plt.plot(ts,test_losses_list,label=\"test\",marker=\"o\",color=\"navy\",linewidth=2)\n",
    "plt.xlabel(\"Interpolation coeff\",fontsize=16)\n",
    "plt.ylabel(\"Loss\",fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('jax_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402007b4c811ecdf694fed2e2753b0f3d5bf9a64e8c58bb62106b798799da515"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
